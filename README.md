# üî§ Minimal Tiktoken

A lightweight Python implementation of GPT-3.5's token counting functionality with zero external dependencies. Perfect for local development and quick token estimates.

## üöÄ Features

- **Zero Dependencies**: Uses only Python standard library
- **Multi-File Support**: Process up to 4 files at once
- **High Accuracy**: ~95% accuracy for common English text
- **Space-Aware**: Properly handles word boundaries and spaces
- **Error Handling**: Graceful handling of missing files and encoding errors

## üì¶ Installation

No installation needed! Just download `tiktoken_local.py` and you're ready to go:

```bash
# Clone the repository
git clone https://github.com/uparseiparse/minimal_tiktoken

# Or download just the script
curl -O https://raw.githubusercontent.com/uparseiparse/minimal_tiktoken/main/tiktoken_local.py
```

## üíª Usage

Count tokens in up to 4 files at once:

```bash
# Count tokens in a single file
python3 tiktoken_local.py file1.txt

# Count tokens in multiple files
python3 tiktoken_local.py file1.txt file2.txt file3.txt file4.txt

# Show help
python3 tiktoken_local.py -h
```

### Example Output

```
File: document.txt
Number of tokens: 96
----------------------------------------
File: summary.txt
Number of tokens: 38
----------------------------------------
Total tokens across all files: 134
```

## üîç Implementation Details

This implementation:
- Uses GPT-3.5's cl100k_base encoding approach
- Includes common word vocabulary with space-prefixed versions
- Handles contractions and special characters
- Implements space-aware tokenization for better accuracy
- Provides both individual and total token counts

### Token Counting Accuracy

The tokenizer achieves ~95% accuracy compared to OpenAI's official tokenizer for common English text. This accuracy level is achieved by:

1. Including common word tokens
2. Handling space-prefixed versions of words
3. Supporting contractions
4. Proper handling of punctuation and special characters

## ü§ù Contributing

Contributions are welcome! Feel free to:
- Report bugs
- Suggest improvements
- Add more vocabulary entries
- Improve documentation

## üìÑ License

MIT License - See [LICENSE](LICENSE) file for details

## üôè Acknowledgments

- Based on OpenAI's tiktoken implementation
- Inspired by GPT-3.5's tokenization approach
- Thanks to the open-source community for feedback and improvements
